{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Read libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "# for nlp\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob\n",
    "\n",
    "# for stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "stemming = PorterStemmer()\n",
    "\n",
    "# for Lemmatizing\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizing = WordNetLemmatizer()\n",
    "\n",
    "# for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# save and load models\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tidy_tweet</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>avg_word</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>numerics</th>\n",
       "      <th>upper_case</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>dysfunctional selfish drags kids dysfunction #run</td>\n",
       "      <td>run</td>\n",
       "      <td>21</td>\n",
       "      <td>102</td>\n",
       "      <td>4.555556</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>thanks #lyft credit cause offer wheelchair van...</td>\n",
       "      <td>lyft  disapointed  getthanked</td>\n",
       "      <td>22</td>\n",
       "      <td>122</td>\n",
       "      <td>5.315789</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>majesty</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>#model</td>\n",
       "      <td>model</td>\n",
       "      <td>17</td>\n",
       "      <td>86</td>\n",
       "      <td>4.928571</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>factsguide society #motivation</td>\n",
       "      <td>motivation</td>\n",
       "      <td>8</td>\n",
       "      <td>39</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49154</th>\n",
       "      <td>49155</td>\n",
       "      <td>NaN</td>\n",
       "      <td>thought factory: left-right polarisation! #tru...</td>\n",
       "      <td>thought factory left right polarisation #trump...</td>\n",
       "      <td>trump  uselections      leadership  politics ...</td>\n",
       "      <td>13</td>\n",
       "      <td>108</td>\n",
       "      <td>8.727273</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49155</th>\n",
       "      <td>49156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>feeling like a mermaid ð #hairflip #neverre...</td>\n",
       "      <td>feeling mermaid #hairflip #neverready #formal ...</td>\n",
       "      <td>hairflip  neverready  formal  wedding  gown  ...</td>\n",
       "      <td>15</td>\n",
       "      <td>96</td>\n",
       "      <td>6.307692</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49156</th>\n",
       "      <td>49157</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#hillary #campaigned today in #ohio((omg)) &amp;am...</td>\n",
       "      <td>#hillary #campaigned #ohio used words assets l...</td>\n",
       "      <td>hillary  campaigned  ohio  omg    clinton  ra...</td>\n",
       "      <td>20</td>\n",
       "      <td>145</td>\n",
       "      <td>7.411765</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49157</th>\n",
       "      <td>49158</td>\n",
       "      <td>NaN</td>\n",
       "      <td>happy, at work conference: right mindset leads...</td>\n",
       "      <td>work conference right mindset leads culture de...</td>\n",
       "      <td>work  mindset</td>\n",
       "      <td>15</td>\n",
       "      <td>104</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49158</th>\n",
       "      <td>49159</td>\n",
       "      <td>NaN</td>\n",
       "      <td>my   song \"so glad\" free download!  #shoegaze ...</td>\n",
       "      <td>song glad free download #shoegaze #newmusic #n...</td>\n",
       "      <td>shoegaze  newmusic  newsong</td>\n",
       "      <td>12</td>\n",
       "      <td>64</td>\n",
       "      <td>5.888889</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49159 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  label                                              tweet  \\\n",
       "0          1    0.0   @user when a father is dysfunctional and is s...   \n",
       "1          2    0.0  @user @user thanks for #lyft credit i can't us...   \n",
       "2          3    0.0                                bihday your majesty   \n",
       "3          4    0.0  #model   i love u take with u all the time in ...   \n",
       "4          5    0.0             factsguide: society now    #motivation   \n",
       "...      ...    ...                                                ...   \n",
       "49154  49155    NaN  thought factory: left-right polarisation! #tru...   \n",
       "49155  49156    NaN  feeling like a mermaid ð #hairflip #neverre...   \n",
       "49156  49157    NaN  #hillary #campaigned today in #ohio((omg)) &am...   \n",
       "49157  49158    NaN  happy, at work conference: right mindset leads...   \n",
       "49158  49159    NaN  my   song \"so glad\" free download!  #shoegaze ...   \n",
       "\n",
       "                                              tidy_tweet  \\\n",
       "0      dysfunctional selfish drags kids dysfunction #run   \n",
       "1      thanks #lyft credit cause offer wheelchair van...   \n",
       "2                                                majesty   \n",
       "3                                                 #model   \n",
       "4                         factsguide society #motivation   \n",
       "...                                                  ...   \n",
       "49154  thought factory left right polarisation #trump...   \n",
       "49155  feeling mermaid #hairflip #neverready #formal ...   \n",
       "49156  #hillary #campaigned #ohio used words assets l...   \n",
       "49157  work conference right mindset leads culture de...   \n",
       "49158  song glad free download #shoegaze #newmusic #n...   \n",
       "\n",
       "                                                 hashtag  word_count  \\\n",
       "0                                                    run          21   \n",
       "1                          lyft  disapointed  getthanked          22   \n",
       "2                                                    NaN           5   \n",
       "3                                                  model          17   \n",
       "4                                             motivation           8   \n",
       "...                                                  ...         ...   \n",
       "49154   trump  uselections      leadership  politics ...          13   \n",
       "49155   hairflip  neverready  formal  wedding  gown  ...          15   \n",
       "49156   hillary  campaigned  ohio  omg    clinton  ra...          20   \n",
       "49157                                      work  mindset          15   \n",
       "49158                        shoegaze  newmusic  newsong          12   \n",
       "\n",
       "       char_count  avg_word  stopwords  hashtags  numerics  upper_case  \n",
       "0             102  4.555556         10         1         0           0  \n",
       "1             122  5.315789          5         3         0           0  \n",
       "2              21  5.666667          1         0         0           0  \n",
       "3              86  4.928571          5         1         0           0  \n",
       "4              39  8.000000          1         1         0           0  \n",
       "...           ...       ...        ...       ...       ...         ...  \n",
       "49154         108  8.727273          0         6         0           0  \n",
       "49155          96  6.307692          1         7         0           0  \n",
       "49156         145  7.411765          3         5         0           0  \n",
       "49157         104  7.500000          2         2         0           0  \n",
       "49158          64  5.888889          1         3         0           0  \n",
       "\n",
       "[49159 rows x 12 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data\n",
    "df = pd.read_csv(\"C:/Users/lenovo/Desktop/hate_speech_detection/data_analyzed.csv\")\n",
    "del df['Unnamed: 0']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Re-split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 49159 entries, 0 to 49158\n",
      "Data columns (total 12 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   id          49159 non-null  int64  \n",
      " 1   label       31962 non-null  float64\n",
      " 2   tweet       49159 non-null  object \n",
      " 3   tidy_tweet  48810 non-null  object \n",
      " 4   hashtag     35894 non-null  object \n",
      " 5   word_count  49159 non-null  int64  \n",
      " 6   char_count  49159 non-null  int64  \n",
      " 7   avg_word    49159 non-null  float64\n",
      " 8   stopwords   49159 non-null  int64  \n",
      " 9   hashtags    49159 non-null  int64  \n",
      " 10  numerics    49159 non-null  int64  \n",
      " 11  upper_case  49159 non-null  int64  \n",
      "dtypes: float64(2), int64(7), object(3)\n",
      "memory usage: 4.5+ MB\n"
     ]
    }
   ],
   "source": [
    "# check data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data based on label\n",
    "train_df = df[0:31962]\n",
    "test_df = df[31962:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After re-splitting**\n",
    "\n",
    "- train data: (31962, 3)\n",
    "- test data: (17197, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 31962 entries, 0 to 31961\n",
      "Data columns (total 12 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   id          31962 non-null  int64  \n",
      " 1   label       31962 non-null  float64\n",
      " 2   tweet       31962 non-null  object \n",
      " 3   tidy_tweet  31751 non-null  object \n",
      " 4   hashtag     23364 non-null  object \n",
      " 5   word_count  31962 non-null  int64  \n",
      " 6   char_count  31962 non-null  int64  \n",
      " 7   avg_word    31962 non-null  float64\n",
      " 8   stopwords   31962 non-null  int64  \n",
      " 9   hashtags    31962 non-null  int64  \n",
      " 10  numerics    31962 non-null  int64  \n",
      " 11  upper_case  31962 non-null  int64  \n",
      "dtypes: float64(2), int64(7), object(3)\n",
      "memory usage: 2.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# chek train dataframe\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important column here is **'tidy_tweet',** which has the pre-processed version of the tweets. We will do verctorization and modeling over this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17197 entries, 31962 to 49158\n",
      "Data columns (total 12 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   id          17197 non-null  int64  \n",
      " 1   label       0 non-null      float64\n",
      " 2   tweet       17197 non-null  object \n",
      " 3   tidy_tweet  17059 non-null  object \n",
      " 4   hashtag     12530 non-null  object \n",
      " 5   word_count  17197 non-null  int64  \n",
      " 6   char_count  17197 non-null  int64  \n",
      " 7   avg_word    17197 non-null  float64\n",
      " 8   stopwords   17197 non-null  int64  \n",
      " 9   hashtags    17197 non-null  int64  \n",
      " 10  numerics    17197 non-null  int64  \n",
      " 11  upper_case  17197 non-null  int64  \n",
      "dtypes: float64(2), int64(7), object(3)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# check test dataframe\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 31751 entries, 0 to 31961\n",
      "Data columns (total 12 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   id          31751 non-null  int64  \n",
      " 1   label       31751 non-null  float64\n",
      " 2   tweet       31751 non-null  object \n",
      " 3   tidy_tweet  31751 non-null  object \n",
      " 4   hashtag     23346 non-null  object \n",
      " 5   word_count  31751 non-null  int64  \n",
      " 6   char_count  31751 non-null  int64  \n",
      " 7   avg_word    31751 non-null  float64\n",
      " 8   stopwords   31751 non-null  int64  \n",
      " 9   hashtags    31751 non-null  int64  \n",
      " 10  numerics    31751 non-null  int64  \n",
      " 11  upper_case  31751 non-null  int64  \n",
      "dtypes: float64(2), int64(7), object(3)\n",
      "memory usage: 3.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# drop rows where tidy_tweet = null\n",
    "train_df = train_df[train_df['tidy_tweet'].notna()]\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data vectorization (for train data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use textual data for predictive modeling, words need to then be encoded as integers, or floating-point values, for use as inputs in machine learning algorithms. This process is called feature extraction (or vectorization). This includes:\n",
    "\n",
    "- Word tokenization\n",
    "- Data normalization\n",
    "    - Word stemming\n",
    "    - Word Lemmatization\n",
    "- Bag of Words (BoW) with train data\n",
    "- Tf - idf with train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tidy_tweet</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>avg_word</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>numerics</th>\n",
       "      <th>upper_case</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>dysfunctional selfish drags kids dysfunction #run</td>\n",
       "      <td>run</td>\n",
       "      <td>21</td>\n",
       "      <td>102</td>\n",
       "      <td>4.555556</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[dysfunctional, selfish, drags, kids, dysfunct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>thanks #lyft credit cause offer wheelchair van...</td>\n",
       "      <td>lyft  disapointed  getthanked</td>\n",
       "      <td>22</td>\n",
       "      <td>122</td>\n",
       "      <td>5.315789</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[thanks, #, lyft, credit, cause, offer, wheelc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>majesty</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[majesty]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>#model</td>\n",
       "      <td>model</td>\n",
       "      <td>17</td>\n",
       "      <td>86</td>\n",
       "      <td>4.928571</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[#, model]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>factsguide society #motivation</td>\n",
       "      <td>motivation</td>\n",
       "      <td>8</td>\n",
       "      <td>39</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[factsguide, society, #, motivation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31957</th>\n",
       "      <td>31958</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ate @user isz that youuu?ðððððð...</td>\n",
       "      <td>youuu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>68</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[youuu]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31958</th>\n",
       "      <td>31959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>to see nina turner on the airwaves trying to...</td>\n",
       "      <td>nina turner airwaves trying wrap mantle genuin...</td>\n",
       "      <td>shame  imwithher</td>\n",
       "      <td>25</td>\n",
       "      <td>131</td>\n",
       "      <td>4.652174</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[nina, turner, airwaves, trying, wrap, mantle,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31959</th>\n",
       "      <td>31960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>listening to sad songs on a monday morning otw...</td>\n",
       "      <td>listening songs monday morning work</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15</td>\n",
       "      <td>63</td>\n",
       "      <td>3.769231</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[listening, songs, monday, morning, work]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31960</th>\n",
       "      <td>31961</td>\n",
       "      <td>1.0</td>\n",
       "      <td>@user #sikh #temple vandalised in in #calgary,...</td>\n",
       "      <td>#sikh #temple vandalised #calgary #wso condemns</td>\n",
       "      <td>sikh  temple  calgary   wso</td>\n",
       "      <td>13</td>\n",
       "      <td>67</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[#, sikh, #, temple, vandalised, #, calgary, #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31961</th>\n",
       "      <td>31962</td>\n",
       "      <td>0.0</td>\n",
       "      <td>thank you @user for you follow</td>\n",
       "      <td>thank follow</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[thank, follow]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31751 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  label                                              tweet  \\\n",
       "0          1    0.0   @user when a father is dysfunctional and is s...   \n",
       "1          2    0.0  @user @user thanks for #lyft credit i can't us...   \n",
       "2          3    0.0                                bihday your majesty   \n",
       "3          4    0.0  #model   i love u take with u all the time in ...   \n",
       "4          5    0.0             factsguide: society now    #motivation   \n",
       "...      ...    ...                                                ...   \n",
       "31957  31958    0.0  ate @user isz that youuu?ðððððð...   \n",
       "31958  31959    0.0    to see nina turner on the airwaves trying to...   \n",
       "31959  31960    0.0  listening to sad songs on a monday morning otw...   \n",
       "31960  31961    1.0  @user #sikh #temple vandalised in in #calgary,...   \n",
       "31961  31962    0.0                   thank you @user for you follow     \n",
       "\n",
       "                                              tidy_tweet  \\\n",
       "0      dysfunctional selfish drags kids dysfunction #run   \n",
       "1      thanks #lyft credit cause offer wheelchair van...   \n",
       "2                                                majesty   \n",
       "3                                                 #model   \n",
       "4                         factsguide society #motivation   \n",
       "...                                                  ...   \n",
       "31957                                              youuu   \n",
       "31958  nina turner airwaves trying wrap mantle genuin...   \n",
       "31959                listening songs monday morning work   \n",
       "31960    #sikh #temple vandalised #calgary #wso condemns   \n",
       "31961                                       thank follow   \n",
       "\n",
       "                              hashtag  word_count  char_count   avg_word  \\\n",
       "0                                 run          21         102   4.555556   \n",
       "1       lyft  disapointed  getthanked          22         122   5.315789   \n",
       "2                                 NaN           5          21   5.666667   \n",
       "3                               model          17          86   4.928571   \n",
       "4                          motivation           8          39   8.000000   \n",
       "...                               ...         ...         ...        ...   \n",
       "31957                             NaN           6          68  12.600000   \n",
       "31958                shame  imwithher          25         131   4.652174   \n",
       "31959                             NaN          15          63   3.769231   \n",
       "31960     sikh  temple  calgary   wso          13          67   5.500000   \n",
       "31961                             NaN           8          32   4.166667   \n",
       "\n",
       "       stopwords  hashtags  numerics  upper_case  \\\n",
       "0             10         1         0           0   \n",
       "1              5         3         0           0   \n",
       "2              1         0         0           0   \n",
       "3              5         1         0           0   \n",
       "4              1         1         0           0   \n",
       "...          ...       ...       ...         ...   \n",
       "31957          1         0         0           0   \n",
       "31958          9         2         0           0   \n",
       "31959          5         0         0           0   \n",
       "31960          2         4         0           0   \n",
       "31961          3         0         0           0   \n",
       "\n",
       "                                                   token  \n",
       "0      [dysfunctional, selfish, drags, kids, dysfunct...  \n",
       "1      [thanks, #, lyft, credit, cause, offer, wheelc...  \n",
       "2                                              [majesty]  \n",
       "3                                             [#, model]  \n",
       "4                   [factsguide, society, #, motivation]  \n",
       "...                                                  ...  \n",
       "31957                                            [youuu]  \n",
       "31958  [nina, turner, airwaves, trying, wrap, mantle,...  \n",
       "31959          [listening, songs, monday, morning, work]  \n",
       "31960  [#, sikh, #, temple, vandalised, #, calgary, #...  \n",
       "31961                                    [thank, follow]  \n",
       "\n",
       "[31751 rows x 13 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['token'] = train_df['tidy_tweet'].apply(lambda x: word_tokenize(x))\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Word stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tidy_tweet</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>avg_word</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>numerics</th>\n",
       "      <th>upper_case</th>\n",
       "      <th>token</th>\n",
       "      <th>tweet_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>dysfunctional selfish drags kids dysfunction #run</td>\n",
       "      <td>run</td>\n",
       "      <td>21</td>\n",
       "      <td>102</td>\n",
       "      <td>4.555556</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[dysfunctional, selfish, drags, kids, dysfunct...</td>\n",
       "      <td>dysfunct selfish drag kid dysfunct # run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>thanks #lyft credit cause offer wheelchair van...</td>\n",
       "      <td>lyft  disapointed  getthanked</td>\n",
       "      <td>22</td>\n",
       "      <td>122</td>\n",
       "      <td>5.315789</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[thanks, #, lyft, credit, cause, offer, wheelc...</td>\n",
       "      <td>thank # lyft credit caus offer wheelchair van ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>majesty</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[majesty]</td>\n",
       "      <td>majesti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>#model</td>\n",
       "      <td>model</td>\n",
       "      <td>17</td>\n",
       "      <td>86</td>\n",
       "      <td>4.928571</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[#, model]</td>\n",
       "      <td># model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>factsguide society #motivation</td>\n",
       "      <td>motivation</td>\n",
       "      <td>8</td>\n",
       "      <td>39</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[factsguide, society, #, motivation]</td>\n",
       "      <td>factsguid societi # motiv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  \\\n",
       "0   1    0.0   @user when a father is dysfunctional and is s...   \n",
       "1   2    0.0  @user @user thanks for #lyft credit i can't us...   \n",
       "2   3    0.0                                bihday your majesty   \n",
       "3   4    0.0  #model   i love u take with u all the time in ...   \n",
       "4   5    0.0             factsguide: society now    #motivation   \n",
       "\n",
       "                                          tidy_tweet  \\\n",
       "0  dysfunctional selfish drags kids dysfunction #run   \n",
       "1  thanks #lyft credit cause offer wheelchair van...   \n",
       "2                                            majesty   \n",
       "3                                             #model   \n",
       "4                     factsguide society #motivation   \n",
       "\n",
       "                          hashtag  word_count  char_count  avg_word  \\\n",
       "0                             run          21         102  4.555556   \n",
       "1   lyft  disapointed  getthanked          22         122  5.315789   \n",
       "2                             NaN           5          21  5.666667   \n",
       "3                           model          17          86  4.928571   \n",
       "4                      motivation           8          39  8.000000   \n",
       "\n",
       "   stopwords  hashtags  numerics  upper_case  \\\n",
       "0         10         1         0           0   \n",
       "1          5         3         0           0   \n",
       "2          1         0         0           0   \n",
       "3          5         1         0           0   \n",
       "4          1         1         0           0   \n",
       "\n",
       "                                               token  \\\n",
       "0  [dysfunctional, selfish, drags, kids, dysfunct...   \n",
       "1  [thanks, #, lyft, credit, cause, offer, wheelc...   \n",
       "2                                          [majesty]   \n",
       "3                                         [#, model]   \n",
       "4               [factsguide, society, #, motivation]   \n",
       "\n",
       "                                       tweet_stemmed  \n",
       "0           dysfunct selfish drag kid dysfunct # run  \n",
       "1  thank # lyft credit caus offer wheelchair van ...  \n",
       "2                                            majesti  \n",
       "3                                            # model  \n",
       "4                          factsguid societi # motiv  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Created one more columns tweet_stemmed it shows tweets' stemmed version\n",
    "train_df['tweet_stemmed'] = train_df['token'].apply(lambda x: ' '.join([stemming.stem(i) for i in x]))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Word Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tidy_tweet</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>avg_word</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>numerics</th>\n",
       "      <th>upper_case</th>\n",
       "      <th>token</th>\n",
       "      <th>tweet_stemmed</th>\n",
       "      <th>tweet_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>dysfunctional selfish drags kids dysfunction #run</td>\n",
       "      <td>run</td>\n",
       "      <td>21</td>\n",
       "      <td>102</td>\n",
       "      <td>4.555556</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[dysfunctional, selfish, drags, kids, dysfunct...</td>\n",
       "      <td>dysfunct selfish drag kid dysfunct # run</td>\n",
       "      <td>dysfunctional selfish drag kid dysfunction # run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>thanks #lyft credit cause offer wheelchair van...</td>\n",
       "      <td>lyft  disapointed  getthanked</td>\n",
       "      <td>22</td>\n",
       "      <td>122</td>\n",
       "      <td>5.315789</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[thanks, #, lyft, credit, cause, offer, wheelc...</td>\n",
       "      <td>thank # lyft credit caus offer wheelchair van ...</td>\n",
       "      <td>thanks # lyft credit cause offer wheelchair va...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>majesty</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[majesty]</td>\n",
       "      <td>majesti</td>\n",
       "      <td>majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>#model</td>\n",
       "      <td>model</td>\n",
       "      <td>17</td>\n",
       "      <td>86</td>\n",
       "      <td>4.928571</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[#, model]</td>\n",
       "      <td># model</td>\n",
       "      <td># model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>factsguide society #motivation</td>\n",
       "      <td>motivation</td>\n",
       "      <td>8</td>\n",
       "      <td>39</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[factsguide, society, #, motivation]</td>\n",
       "      <td>factsguid societi # motiv</td>\n",
       "      <td>factsguide society # motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  \\\n",
       "0   1    0.0   @user when a father is dysfunctional and is s...   \n",
       "1   2    0.0  @user @user thanks for #lyft credit i can't us...   \n",
       "2   3    0.0                                bihday your majesty   \n",
       "3   4    0.0  #model   i love u take with u all the time in ...   \n",
       "4   5    0.0             factsguide: society now    #motivation   \n",
       "\n",
       "                                          tidy_tweet  \\\n",
       "0  dysfunctional selfish drags kids dysfunction #run   \n",
       "1  thanks #lyft credit cause offer wheelchair van...   \n",
       "2                                            majesty   \n",
       "3                                             #model   \n",
       "4                     factsguide society #motivation   \n",
       "\n",
       "                          hashtag  word_count  char_count  avg_word  \\\n",
       "0                             run          21         102  4.555556   \n",
       "1   lyft  disapointed  getthanked          22         122  5.315789   \n",
       "2                             NaN           5          21  5.666667   \n",
       "3                           model          17          86  4.928571   \n",
       "4                      motivation           8          39  8.000000   \n",
       "\n",
       "   stopwords  hashtags  numerics  upper_case  \\\n",
       "0         10         1         0           0   \n",
       "1          5         3         0           0   \n",
       "2          1         0         0           0   \n",
       "3          5         1         0           0   \n",
       "4          1         1         0           0   \n",
       "\n",
       "                                               token  \\\n",
       "0  [dysfunctional, selfish, drags, kids, dysfunct...   \n",
       "1  [thanks, #, lyft, credit, cause, offer, wheelc...   \n",
       "2                                          [majesty]   \n",
       "3                                         [#, model]   \n",
       "4               [factsguide, society, #, motivation]   \n",
       "\n",
       "                                       tweet_stemmed  \\\n",
       "0           dysfunct selfish drag kid dysfunct # run   \n",
       "1  thank # lyft credit caus offer wheelchair van ...   \n",
       "2                                            majesti   \n",
       "3                                            # model   \n",
       "4                          factsguid societi # motiv   \n",
       "\n",
       "                                    tweet_lemmatized  \n",
       "0   dysfunctional selfish drag kid dysfunction # run  \n",
       "1  thanks # lyft credit cause offer wheelchair va...  \n",
       "2                                            majesty  \n",
       "3                                            # model  \n",
       "4                    factsguide society # motivation  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['tweet_lemmatized'] = train_df['token'].apply(lambda x: ' '.join([lemmatizing.lemmatize(i) for i in x]))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Bag of Words (BoW) with train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=0.9, max_features=1000, min_df=2,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "bow_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<31751x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 105535 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bag-of-words stemmed\n",
    "trainbow_stem = bow_vectorizer.fit_transform(train_df['tweet_stemmed'])\n",
    "trainbow_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainbow_stem.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<31751x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 95459 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bow lemmatized\n",
    "trainbow_lemm = bow_vectorizer.fit_transform(train_df['tweet_lemmatized'])\n",
    "trainbow_lemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainbow_lemm.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf - idf with train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=0.9, max_features=1000,\n",
       "                min_df=2, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words='english', strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf-idf stemmed\n",
    "traintfidf_stem = tfidf_vectorizer.fit_transform(train_df['tweet_stemmed'])\n",
    "traintfidf_stem.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf-idf lemmatized\n",
    "traintfidf_lemm = tfidf_vectorizer.fit_transform(train_df['tweet_lemmatized'])\n",
    "traintfidf_lemm.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data vectorization (for test data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This includes:\n",
    "    - Word tokenization\n",
    "    - Word stemming\n",
    "    - Word Lemmatization\n",
    "    - BoW with test data\n",
    "    - TF-IDF with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tidy_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31962</th>\n",
       "      <td>31963</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31963</th>\n",
       "      <td>31964</td>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "      <td>#white #supremacists everyone #birds #movie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet  \\\n",
       "31962  31963  #studiolife #aislife #requires #passion #dedic...   \n",
       "31963  31964   @user #white #supremacists want everyone to s...   \n",
       "\n",
       "                                              tidy_tweet  \n",
       "31962  #studiolife #aislife #requires #passion #dedic...  \n",
       "31963        #white #supremacists everyone #birds #movie  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature selection of few columns\n",
    "test_df = test_df[['id','tweet','tidy_tweet']]\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only the rows where data is not NA:\n",
    "test_df = test_df[test_df['tidy_tweet'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 17059 entries, 31962 to 49158\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   id          17059 non-null  int64 \n",
      " 1   tweet       17059 non-null  object\n",
      " 2   tidy_tweet  17059 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 533.1+ KB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Word tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating token for the clean tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tidy_tweet</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31962</th>\n",
       "      <td>31963</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "      <td>[#, studiolife, #, aislife, #, requires, #, pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31963</th>\n",
       "      <td>31964</td>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "      <td>#white #supremacists everyone #birds #movie</td>\n",
       "      <td>[#, white, #, supremacists, everyone, #, birds...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet  \\\n",
       "31962  31963  #studiolife #aislife #requires #passion #dedic...   \n",
       "31963  31964   @user #white #supremacists want everyone to s...   \n",
       "\n",
       "                                              tidy_tweet  \\\n",
       "31962  #studiolife #aislife #requires #passion #dedic...   \n",
       "31963        #white #supremacists everyone #birds #movie   \n",
       "\n",
       "                                                   token  \n",
       "31962  [#, studiolife, #, aislife, #, requires, #, pa...  \n",
       "31963  [#, white, #, supremacists, everyone, #, birds...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['token'] = test_df['tidy_tweet'].apply(lambda x: word_tokenize(x))\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Word stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tidy_tweet</th>\n",
       "      <th>token</th>\n",
       "      <th>tweet_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31962</th>\n",
       "      <td>31963</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "      <td>[#, studiolife, #, aislife, #, requires, #, pa...</td>\n",
       "      <td># studiolif # aislif # requir # passion # dedi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31963</th>\n",
       "      <td>31964</td>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "      <td>#white #supremacists everyone #birds #movie</td>\n",
       "      <td>[#, white, #, supremacists, everyone, #, birds...</td>\n",
       "      <td># white # supremacist everyon # bird # movi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet  \\\n",
       "31962  31963  #studiolife #aislife #requires #passion #dedic...   \n",
       "31963  31964   @user #white #supremacists want everyone to s...   \n",
       "\n",
       "                                              tidy_tweet  \\\n",
       "31962  #studiolife #aislife #requires #passion #dedic...   \n",
       "31963        #white #supremacists everyone #birds #movie   \n",
       "\n",
       "                                                   token  \\\n",
       "31962  [#, studiolife, #, aislife, #, requires, #, pa...   \n",
       "31963  [#, white, #, supremacists, everyone, #, birds...   \n",
       "\n",
       "                                           tweet_stemmed  \n",
       "31962  # studiolif # aislif # requir # passion # dedi...  \n",
       "31963        # white # supremacist everyon # bird # movi  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Created one more columns tweet_stemmed it shows tweets' stemmed version\n",
    "test_df['tweet_stemmed'] = test_df['token'].apply(lambda x: ' '.join([stemming.stem(i) for i in x]))\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Word Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tidy_tweet</th>\n",
       "      <th>token</th>\n",
       "      <th>tweet_stemmed</th>\n",
       "      <th>tweet_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31962</th>\n",
       "      <td>31963</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "      <td>[#, studiolife, #, aislife, #, requires, #, pa...</td>\n",
       "      <td># studiolif # aislif # requir # passion # dedi...</td>\n",
       "      <td># studiolife # aislife # requires # passion # ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31963</th>\n",
       "      <td>31964</td>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "      <td>#white #supremacists everyone #birds #movie</td>\n",
       "      <td>[#, white, #, supremacists, everyone, #, birds...</td>\n",
       "      <td># white # supremacist everyon # bird # movi</td>\n",
       "      <td># white # supremacist everyone # bird # movie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet  \\\n",
       "31962  31963  #studiolife #aislife #requires #passion #dedic...   \n",
       "31963  31964   @user #white #supremacists want everyone to s...   \n",
       "\n",
       "                                              tidy_tweet  \\\n",
       "31962  #studiolife #aislife #requires #passion #dedic...   \n",
       "31963        #white #supremacists everyone #birds #movie   \n",
       "\n",
       "                                                   token  \\\n",
       "31962  [#, studiolife, #, aislife, #, requires, #, pa...   \n",
       "31963  [#, white, #, supremacists, everyone, #, birds...   \n",
       "\n",
       "                                           tweet_stemmed  \\\n",
       "31962  # studiolif # aislif # requir # passion # dedi...   \n",
       "31963        # white # supremacist everyon # bird # movi   \n",
       "\n",
       "                                        tweet_lemmatized  \n",
       "31962  # studiolife # aislife # requires # passion # ...  \n",
       "31963      # white # supremacist everyone # bird # movie  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['tweet_lemmatized'] = test_df['token'].apply(lambda x: ' '.join([lemmatizing.lemmatize(i) for i in x]))\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=0.9, max_features=1000, min_df=2,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing library\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "bow_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  BoW with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bow stemmed (test data)\n",
    "testbow_stem = bow_vectorizer.fit_transform(test_df['tweet_stemmed'])\n",
    "testbow_stem.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bow lemmatized (test data)\n",
    "testbow_lemm = bow_vectorizer.fit_transform(test_df['tweet_lemmatized'])\n",
    "testbow_lemm.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  TF-IDF with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=0.9, max_features=1000,\n",
       "                min_df=2, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words='english', strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf-idf stemmed (test data)\n",
    "testtfidf_stem = tfidf_vectorizer.fit_transform(test_df['tweet_stemmed'])\n",
    "testtfidf_stem.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf-idf lemmatized (test data)\n",
    "testtfidf_lemm = tfidf_vectorizer.fit_transform(test_df['tweet_lemmatized'])\n",
    "testtfidf_lemm.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Dataset into Train and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=traintfidf_lemm #x: predictors\n",
    "y=train_df['label'] #y: label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain,xtest,ytrain,ytest=train_test_split(X,y,test_size=.3,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr=LogisticRegression() # for lemmatized data\n",
    "lr.fit(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_lr=lr.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score : 0.9455175309678774\n",
      "f1 score : 0.4188129899216126\n",
      "[[8820  479]\n",
      " [  40  187]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.95      0.97      9299\n",
      "         1.0       0.28      0.82      0.42       227\n",
      "\n",
      "    accuracy                           0.95      9526\n",
      "   macro avg       0.64      0.89      0.70      9526\n",
      "weighted avg       0.98      0.95      0.96      9526\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy score :\", accuracy_score(predict_lr,ytest))\n",
    "\n",
    "# calculating the f1 score for the validation set\n",
    "print(\"f1 score :\", f1_score(predict_lr,ytest))\n",
    "\n",
    "print(confusion_matrix(predict_lr,ytest))\n",
    "print(classification_report(predict_lr,ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1=traintfidf_stem # for stemmed data\n",
    "y1=train_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1train,x1test,y1train,y1test=train_test_split(X1,y1,test_size=.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr1=LogisticRegression()\n",
    "lr1.fit(x1train,y1train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_lr1=lr1.predict(x1test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score : 0.94625236195675\n",
      "f1 score : 0.4260089686098655\n",
      "[[8824  476]\n",
      " [  36  190]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.95      0.97      9300\n",
      "         1.0       0.29      0.84      0.43       226\n",
      "\n",
      "    accuracy                           0.95      9526\n",
      "   macro avg       0.64      0.89      0.70      9526\n",
      "weighted avg       0.98      0.95      0.96      9526\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# accuracy score\n",
    "print(\"accuracy score :\", accuracy_score(predict_lr1,y1test))\n",
    "\n",
    "# calculating the f1 score for the validation set\n",
    "print(\"f1 score :\", f1_score(predict_lr1,ytest))\n",
    "\n",
    "print(confusion_matrix(predict_lr1,y1test))\n",
    "print(classification_report(predict_lr1,y1test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:02:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier()\n",
    "xgb.fit(xtrain.toarray(),ytrain)\n",
    "predict_xgb=xgb.predict(xtest.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score : 0.9473021205122821\n",
      "f1 score : 0.4531590413943355\n",
      "[[8816  458]\n",
      " [  44  208]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.95      0.97      9274\n",
      "         1.0       0.31      0.83      0.45       252\n",
      "\n",
      "    accuracy                           0.95      9526\n",
      "   macro avg       0.65      0.89      0.71      9526\n",
      "weighted avg       0.98      0.95      0.96      9526\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# accuracy score\n",
    "print(\"accuracy score :\", accuracy_score(predict_xgb,ytest))\n",
    "\n",
    "# calculating the f1 score for the validation set\n",
    "print(\"f1 score :\", f1_score(predict_xgb,ytest))\n",
    "\n",
    "print(confusion_matrix(predict_xgb,ytest))\n",
    "print(classification_report(predict_xgb,ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(xtrain.toarray(),ytrain)\n",
    "predict_dt = dt.predict(xtest.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score : 0.9438379172790258\n",
      "f1 score : 0.5327510917030568\n",
      "[[8686  361]\n",
      " [ 174  305]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.96      0.97      9047\n",
      "         1.0       0.46      0.64      0.53       479\n",
      "\n",
      "    accuracy                           0.94      9526\n",
      "   macro avg       0.72      0.80      0.75      9526\n",
      "weighted avg       0.95      0.94      0.95      9526\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# accuracy score\n",
    "print(\"accuracy score :\", accuracy_score(predict_dt,ytest))\n",
    "\n",
    "# calculating the f1 score for the validation set\n",
    "print(\"f1 score :\", f1_score(predict_dt,ytest))\n",
    "\n",
    "print(confusion_matrix(predict_dt,ytest))\n",
    "print(classification_report(predict_dt,ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(xtrain.toarray(),ytrain) # you can test with grid search methodology\n",
    "predict_rf = rf.predict(xtest.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score : 0.9525509132899433\n",
      "f1 score : 0.5670498084291188\n",
      "[[8778  370]\n",
      " [  82  296]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.96      0.97      9148\n",
      "         1.0       0.44      0.78      0.57       378\n",
      "\n",
      "    accuracy                           0.95      9526\n",
      "   macro avg       0.72      0.87      0.77      9526\n",
      "weighted avg       0.97      0.95      0.96      9526\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# accuracy score\n",
    "print(\"accuracy score :\", accuracy_score(predict_rf,ytest))\n",
    "\n",
    "# calculating the f1 score for the validation set\n",
    "print(\"f1 score :\", f1_score(predict_rf,ytest))\n",
    "\n",
    "print(confusion_matrix(predict_rf,ytest))\n",
    "print(classification_report(predict_rf,ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forst model** has given us the best performance so far in terms of F1-score and accuracy. Let’s try to do predictions using the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forst\n",
    "test_predict_rf = rf.predict(testtfidf_lemm)\n",
    "test_predict_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tidy_tweet</th>\n",
       "      <th>token</th>\n",
       "      <th>tweet_stemmed</th>\n",
       "      <th>tweet_lemmatized</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31962</th>\n",
       "      <td>31963</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "      <td>[#, studiolife, #, aislife, #, requires, #, pa...</td>\n",
       "      <td># studiolif # aislif # requir # passion # dedi...</td>\n",
       "      <td># studiolife # aislife # requires # passion # ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31963</th>\n",
       "      <td>31964</td>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "      <td>#white #supremacists everyone #birds #movie</td>\n",
       "      <td>[#, white, #, supremacists, everyone, #, birds...</td>\n",
       "      <td># white # supremacist everyon # bird # movi</td>\n",
       "      <td># white # supremacist everyone # bird # movie</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31964</th>\n",
       "      <td>31965</td>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
       "      <td>safe ways heal #acne #altwaystoheal #healing</td>\n",
       "      <td>[safe, ways, heal, #, acne, #, altwaystoheal, ...</td>\n",
       "      <td>safe way heal # acn # altwaystoh # heal</td>\n",
       "      <td>safe way heal # acne # altwaystoheal # healing</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31965</th>\n",
       "      <td>31966</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "      <td>cursed child book reservations already #harryp...</td>\n",
       "      <td>[cursed, child, book, reservations, already, #...</td>\n",
       "      <td>curs child book reserv alreadi # harrypott # p...</td>\n",
       "      <td>cursed child book reservation already # harryp...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31966</th>\n",
       "      <td>31967</td>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
       "      <td>#bihday amazing hilarious #nephew ahmir uncle ...</td>\n",
       "      <td>[#, bihday, amazing, hilarious, #, nephew, ahm...</td>\n",
       "      <td># bihday amaz hilari # nephew ahmir uncl dave ...</td>\n",
       "      <td># bihday amazing hilarious # nephew ahmir uncl...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet  \\\n",
       "31962  31963  #studiolife #aislife #requires #passion #dedic...   \n",
       "31963  31964   @user #white #supremacists want everyone to s...   \n",
       "31964  31965  safe ways to heal your #acne!!    #altwaystohe...   \n",
       "31965  31966  is the hp and the cursed child book up for res...   \n",
       "31966  31967    3rd #bihday to my amazing, hilarious #nephew...   \n",
       "\n",
       "                                              tidy_tweet  \\\n",
       "31962  #studiolife #aislife #requires #passion #dedic...   \n",
       "31963        #white #supremacists everyone #birds #movie   \n",
       "31964       safe ways heal #acne #altwaystoheal #healing   \n",
       "31965  cursed child book reservations already #harryp...   \n",
       "31966  #bihday amazing hilarious #nephew ahmir uncle ...   \n",
       "\n",
       "                                                   token  \\\n",
       "31962  [#, studiolife, #, aislife, #, requires, #, pa...   \n",
       "31963  [#, white, #, supremacists, everyone, #, birds...   \n",
       "31964  [safe, ways, heal, #, acne, #, altwaystoheal, ...   \n",
       "31965  [cursed, child, book, reservations, already, #...   \n",
       "31966  [#, bihday, amazing, hilarious, #, nephew, ahm...   \n",
       "\n",
       "                                           tweet_stemmed  \\\n",
       "31962  # studiolif # aislif # requir # passion # dedi...   \n",
       "31963        # white # supremacist everyon # bird # movi   \n",
       "31964            safe way heal # acn # altwaystoh # heal   \n",
       "31965  curs child book reserv alreadi # harrypott # p...   \n",
       "31966  # bihday amaz hilari # nephew ahmir uncl dave ...   \n",
       "\n",
       "                                        tweet_lemmatized  label  \n",
       "31962  # studiolife # aislife # requires # passion # ...    0.0  \n",
       "31963      # white # supremacist everyone # bird # movie    0.0  \n",
       "31964     safe way heal # acne # altwaystoheal # healing    0.0  \n",
       "31965  cursed child book reservation already # harryp...    0.0  \n",
       "31966  # bihday amazing hilarious # nephew ahmir uncl...    1.0  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['label'] = test_predict_rf\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    16502\n",
       "1.0      557\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "# we give what ever name in fist line (the model will be stored in that name)\n",
    "# in second line we provide the name of our model (which is classifier in our case)\n",
    "\n",
    "import pickle\n",
    "RVC_filename = 'finalized_RFC_model.sav' # finalized_RFC_model: is the new model name\n",
    "pickle.dump(rf, open(RVC_filename, 'wb')) # rf: random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model will be saved in the current directory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tfidf vectorizer\n",
    "# Save fit vectorizer and fit tfidftransformer, use in prediction\n",
    "\n",
    "tfidftransformer_path = 'tfidf-vectorizer.pkl' # tfidf-vectorizer is the new vectorizer name\n",
    "with open(tfidftransformer_path, 'wb') as fw:\n",
    "    pickle.dump(traintfidf_lemm, fw) # traintfidf_lemm: vectorizer name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
